# AI that Works for Music, Not Against It {#sec-ai}

The European Parliament’s resolution on the music streaming market warns of the risks that AI-generated content poses for *discoverability, attribution, and fair remuneration* if metadata remains incomplete or unreliable. At the same time, the *Music Ecosystem 2025* study highlights that AI will be both a disruption and an opportunity: while it can overwhelm systems with synthetic material, it also offers tools to automate documentation, reduce costs, and strengthen evidence-based policymaking [@music_ecossytem_2025, pp.23–24].

![](png/AI/trustworthy_AI_in_music.png){fig-align="center"}

Artificial intelligence is therefore central to the future of Europe’s music ecosystem. On one hand, it threatens to exacerbate existing inequalities by concentrating technological advantages in platforms and major rights holders. On the other, it can repair, enrich, and automate processes that are otherwise prohibitively costly for small actors. The challenge is not whether AI will be used, but whether its benefits will be distributed fairly across the ecosystem.

European policy provides guidance for this balancing act. The *Ethics Guidelines for Trustworthy AI* underline that AI must be lawful, ethical, and robust throughout its lifecycle [@ethics_guidelines_trustworthy_ai_2019]. The *Getting the Future Right* report by the Fundamental Rights Agency stresses the need to align AI with fundamental rights, especially where vulnerable groups and cultural participation are concerned [@getting_the_future_right_2020]. Most recently, the **AI Act** enshrines a risk-based regulatory framework, defining obligations for providers and deployers of AI systems while reaffirming the principles of subsidiarity and proportionality in EU digital policy [@ai_act_2024].

Against this backdrop, the Observatory proposes AI not as a substitute for human creativity or governance, but as a **shared utility**: a way to pool curative, agentic, and generative services within a federated infrastructure. This ensures that SMEs, non-profits, and community archives gain access to trustworthy AI capacities, reducing costs and risks while preserving diversity and accountability in the European music ecosystem.

### Executive summary (what we propose)

**Scope:** Top layer: applied services that ride on the data space (curative, agentic, and generative uses); shared AI capacity so SMEs/NGOs aren’t forced to hire scarce talent. - Create *shared AI services* (not bespoke in-house teams) for entity recognition, duplicate detection, cross-catalogue matching, multilingual enrichment, and compliance monitoring.\
- Educate stakeholders using a simple three-part model of AI (agentic/generative/curative) tied to concrete tasks and guardrails.\
- Reduce operational and capital costs by embedding AI into metadata, claims, and documentation workflows.\
- Support creators by embedding metadata from the point of creation, extending the Parliament’s “metadata from birth” principle.

## Discussion

-   EP resolution on AI-generated flooding and discoverability/remuneration risks [@ep_resolution_music_streaming_2024]; ecosystem report on AI as disruption *and* opportunity.\
-   Practical gaps: SMEs/CMOs lack AI engineers; asymmetry with platform capabilities.

The following subsections highlight lessons from international practice and concrete cases where AI can reduce costs, improve data quality, and create more equitable access to metadata services.

### The U.S. Mechanical Licensing Collective (MLC) as a Metadata Clearinghouse

The *Mechanical Licensing Collective* (MLC) was created under the U.S. *Music Modernization Act* (2018) and began operations in January 2021. It administers a **blanket mechanical license** for digital streaming and download services, replacing fragmented song-by-song licensing. Its mandate is to ensure that songwriters, lyricists, composers, and publishers receive timely mechanical royalties, while providing transparency through a public works database and member portal [@mlc_annual_report_2021].

From its inception, the MLC inherited more than **\$424 million in unmatched royalties** from digital service providers (DSPs)—funds that could not be allocated because works were not properly registered or matched. These unmatched sums must eventually be distributed, and if they cannot be claimed, they are paid out to publishers on a market-share basis. For independent songwriters, this creates both an opportunity and a risk: registration with the MLC is free, but failure to register means royalties may be permanently lost.

The MLC highlights a systemic lesson: identifiers like ISWC and ISRC must be correctly captured and maintained at source, or royalties remain trapped in the “black box.” To address this, the MLC has developed **matching and reconciliation routines** and offers search tools so creators can check whether their works are properly registered. Education remains a critical challenge, since many independent songwriters incorrectly assume that affiliation with ASCAP, BMI, or SESAC (performance rights organisations) covers mechanical royalties as well.

By late 2022, the MLC had already distributed nearly **\$700 million in royalties**, but its operations also illustrate the fragility of metadata-dependent systems. In 2025, it filed—and lost—a high-profile lawsuit against Spotify, which had sought to classify audiobooks within its music service to reduce royalty obligations [@varghese_blackbox_2024]. This underscores that even with centralised licensing, **governance and enforcement remain contested**.

For European debates, the MLC demonstrates how a large-scale, rights-compliant clearinghouse can consolidate reporting, improve metadata quality, and distribute royalties more transparently. Yet it also shows the **limits of centralisation**: creators still need to actively claim and maintain their records, education gaps persist, and disputes between collective agencies and global platforms remain unresolved.

### Economies of Scale in Metadata

Large platforms and major labels manage to document millions of assets in parallel, achieving economies of scale that smaller actors cannot match. By contrast, for SMEs, non-profits, or community archives, the cost of documentation per asset is disproportionately high, often exceeding the commercial value of the repertoire. This gap explains why so many “frozen” assets remain unregistered and invisible in the digital ecosystem.

Agentic AI, if deployed within a shared knowledge base and aligned with modular ontologies in a federated data sharing space, can reduce operational expenditure (OPEX) by automating repetitive documentation tasks. This would allow smaller players to benefit from the same scale effects as the global platforms, without compromising quality or compliance. The frozen asset case is only the most visible example: AI-enabled economies of scale would lower costs across the entire long tail of music assets.

### Unfreezing Frozen Assets

Many music assets remain “frozen” because their documentation costs exceed their current commercial value. This applies to non-commercial repertoires, small-label releases, and culturally valuable but low-market recordings. Without affordable workflows, these works cannot enter modern distribution systems, regardless of their cultural or artistic significance.

The *Unlabel* pilot illustrates this problem: by treating catalogue transfers and documentation as high-cost, high-friction processes, valuable repertoires remain locked away. AI-assisted metadata repair and DDEX-compliant catalogue transfer workflows provide a pathway to lower costs and bring neglected repertoires back into circulation.

### AI and Smarter Capital Expenditure

For small-to-medium actors such as CMOs, music information centres, and libraries, IT systems are a major capital expense. Many run on amortised or obsolete infrastructure that is expensive to maintain, upgrade, or replace. Traditional CAPEX planning assumes periodic replacement cycles, but this is not realistic for institutions with constrained budgets.

Curative and reparative AI can extend the useful life of such systems by translating, reconciling, and enriching outputs from outdated software. For example, an AI service can process legacy royalty accounting exports, standardise them against current identifier regimes, and feed them into observatory workflows. This reduces the urgency of costly IT replacement while preserving interoperability.

### AI Support for Investment Into New Repertoire Assets

While generative AI that disregards human repertoires can undermine cultural value, AI also has constructive roles. Just as photographers benefit from embedded AI in tools like Photoshop or GIMP, musicians and producers can use AI to reduce the costs of composition, recording, and documentation. In practice, this means that creating new works and registering them with identifiers can become less burdensome and more accessible.

This perspective aligns with the European Parliament’s call for “metadata from birth” [@ep_resolution_music_streaming_2024], but it goes further. AI can not only generate metadata automatically at the moment of creation, but also support sound recording, scoring, and archiving processes directly, ensuring that new assets enter circulation with complete, interoperable metadata.

### Embedding AI in Creative Workflows Instead of Replacing Creatives

We propose to embed AI services directly into music creation and distribution tools, so that metadata is generated and validated as part of the creative process. These services should respect human authorship and repertoires, while reducing costs for creators by automating documentation and identifier allocation. Public support could incentivise software vendors and open-source projects to integrate such features, ensuring that SMEs, independent artists, and non-commercial creators also benefit. This would extend the Parliament’s “metadata from birth” principle into a broader vision of *creation with embedded metadata*, lowering barriers for creators and strengthening the metadata foundations of the European music ecosystem.

## AI for Working Capital Optimisation

For many rightsholders, especially smaller ones, cash flow is delayed because of slow or inefficient links between their systems and platforms, or between authors, publishers, and collective management organisations. Uploads, claims, and distributions are often handled through manual, fragmented processes that increase transaction costs and tie up working capital.

AI can support working capital optimisation by enabling cheaper API-based liaisons with platforms and CMOs, automating the management of routine claims, and improving the matching of works and recordings. This reduces delays and transaction friction, allowing creators and small organisations to access revenue more quickly.

## Policy Proposals

-   Operate pooled “AI utilities” behind clear SLAs: reconciliation-as-a-service; watchlists for plagiarism/near-duplicates; machine-assisted translations/summaries for archival deposits.\
-   Governance/ethics by design: audit logs, explainability notes, error budgets; opt-in data sharing; “green list” of permissible automations per partner.\
-   Standards/regulation/public investment inline: model/data licensing; consent and transparency; targeted public funding for shared services.

### Curative AI as a Remediation Solution

While data sharing spaces establish the rules and structures for *new* data flows, they do not address the **legacy backlog** of poorly formatted or incomplete open data already in circulation. Here, **curative AI** provides a complementary solution. For the music ecosystem — and for public-sector bodies obliged under the *Open Data Directive* to release high-value datasets — such AI applications should be prioritised.

AI-assisted services can automatically detect duplicates, infer missing identifiers, and reconcile heterogeneous formats. They can enrich metadata with multilingual descriptions, cross-link authority files (e.g. ISNI, VIAF, ORCID), and repair legacy exports from outdated IT systems. In effect, they transform datasets that are legally open but practically unusable into resources that can circulate across the ecosystem.

In practical terms, curative AI reduces the hidden costs of “free” data. Even when datasets are available at zero monetary price, acquisition, transformation, and integration impose substantial burdens [@schnurr_ogd_markets_2021]. By pooling remediation services at the level of the Observatory — rather than leaving each SME or archive to hire scarce AI engineers — the sector can benefit from economies of scale and achieve more consistent outcomes.

Thus, governance and remediation are two sides of the same coin:\
- **Data sharing spaces** ensure that *new* data is created and exchanged in interoperable ways.\
- **Curative AI** repairs the *inherited stock* of legacy and low-quality datasets.

Together, they close the gap between the *right of reuse* granted by the Open Data Directive and the *means of reuse* required for music, culture, and AI-driven innovation.[^ai-1]

[^ai-1]: Empirical studies underline that legal openness alone rarely produces machine-actionable reuse. Without prescriptive standards, high-value datasets are published in divergent formats and with inconsistent metadata [@klimek_stirdata_2023, p.184]. Reference models and standards can mitigate this going forward [@noardo_standards_2024], but existing datasets often require intensive remediation. Curative AI provides this remediation by automating anomaly detection, schema transformations, and semantic enrichment. For example, Ni et al. demonstrate that machine-learning frameworks can flag outliers, propose corrected values, and normalise structures at scale, significantly reducing manual repair costs [@ni_automatic_repair_2023, p.3]. In cultural and company register contexts, pilots such as STIRData have shown how AI-driven pipelines transform heterogeneous exports into interoperable, HVD-compliant formats [@klimek_stirdata_2023, p.185]. These examples illustrate why curative AI, deployed as a shared Observatory service, is indispensable for turning open data from a *legal right* into a *practical resource*.

### Lowering Documentation Barriers

We propose to adapt *Unlabel*’s approach as a model for unfreezing frozen assets. By leveraging AI-assisted metadata repair and DDEX-compliant catalogue transfer workflows, documentation costs can be reduced enough to enable non-profits, small labels, and community archives to register and redistribute neglected repertoires. Public support should be directed to subsidise initial onboarding costs, create standardised transfer pipelines, and incentivise low-friction reuse of metadata across systems. This would extend the benefits of metadata interoperability to cultural assets currently excluded from the digital market.

### Shared AI Services for Scale Efficiency

We propose to establish AI-enabled metadata services within European data sharing spaces, governed by shared ontologies and ethical guardrails. These services should automate routine documentation tasks (identifier reconciliation, crosswalks, enrichment from external knowledge bases) and distribute the resulting efficiencies across all actors, not just the majors. Public investment should support the creation of open, agentic AI modules that plug into observatory infrastructures and enable SMEs, non-profits, and independent creators to achieve platform-level economies of scale. This would not only unfreeze frozen assets but systematically reduce the OPEX of music metadata management across Europe.

### Redirecting CAPEX Toward Knowledge Capital

We propose a funding and governance model that prioritises investment in *knowledge capital*—shared ontologies, pattern libraries, and AI modules—rather than continuous replacement of IT assets. By deploying AI to extend the functionality of legacy systems, CMOs and cultural institutions can redirect scarce capital away from hardware/software churn and into data quality, interoperability, and human expertise. This reallocation of CAPEX would create more durable value: instead of buying new IT every five years, institutions would invest in collective intelligence that strengthens the entire music ecosystem.

### AI-Powered Claims and API Integration

We propose to develop AI-assisted services for claims management and API integration across the European music ecosystem. By lowering the cost of system-to-system communication, these tools would give European SMEs and rightsholders the same efficiencies that global platforms already enjoy. The US **Mechanical Licensing Collective (MLC)** has demonstrated how coordinated metadata and streamlined claiming processes can accelerate payouts and reduce disputes [@varghese_blackbox_2024]. Europe should build on this model by investing in shared, AI-enabled liaison services that optimise working capital for creators, CMOs, and independent labels alike.
